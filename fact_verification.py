# fact_verification.py

import os
import re
import logging
from typing import List, Tuple, Dict, Any, Union # Added Union
from dotenv import load_dotenv
import google.generativeai as genai
from PIL import Image, UnidentifiedImageError

# --- Add Warning Suppression ---
import warnings
try:
    from huggingface_hub.file_download import CCompletionProgress
    warnings.filterwarnings("ignore", category=FutureWarning, module='huggingface_hub.file_download')
    logging.info("Suppressed specific huggingface_hub FutureWarning.")
except ImportError:
    warnings.filterwarnings("ignore", category=FutureWarning)
    logging.info("Suppressed general FutureWarning (huggingface_hub specific import failed).")

# --- Logging Setup (Basic) ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s:%(filename)s:%(lineno)d: %(message)s')

# --- Environment Setup ---
load_dotenv()
GEMINI_API_KEY: str | None = os.getenv("GEMINI_API_KEY")
llm_model: genai.GenerativeModel | None = None

if not GEMINI_API_KEY:
    logging.critical("GEMINI_API_KEY environment variable not set. AI features will be unavailable.")
else:
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        MODEL_NAME: str = os.getenv("GEMINI_MODEL", "gemini-1.5-flash-latest")
        logging.info(f"Configuring Gemini AI Model: {MODEL_NAME}...")
        llm_model = genai.GenerativeModel(MODEL_NAME)
        logging.info(f"Gemini AI Model configured successfully.")
    except Exception as e:
        logging.fatal(f"Error configuring Gemini AI with key: {e}", exc_info=True)

# --- Function Definitions ---

# Define return type for sources
SourceInfo = Dict[str, str] # {'title': '...', 'url': '...'}

def verify_fact(claim: str) -> Tuple[str, str, List[SourceInfo]]: # Updated return type
    """
    Verify a simple text claim using the configured LLM.
    Returns: truth_score (str), explanation (str), sources (list of {'title': str, 'url': str})
    """
    if not llm_model:
        logging.warning("verify_fact called but LLM model is not available.")
        return "N/A", "LLM model is not configured or failed to load.", []

    # --- UPDATED PROMPT: Ask for Reasoning first, then Title+URL for sources ---
    prompt = f"""
    Your task is to rigorously analyze the following claim for factual accuracy based on current, verifiable knowledge from high-quality sources, up to your last update. Identify if the claim is a statement of fact or opinion. Prioritize recent information where relevant.

    Claim: "{claim}"

    Instructions:
    1.  **Reasoning:** First, explain your step-by-step reasoning process in detail. Cover the evidence found (or lack thereof) from reliable sources, mention conflicting information or context, state if it's opinion-based or lacks evidence. Aim for objectivity. Conclude this section clearly.

    --- END REASONING --- [Marker]

    2.  **Formatted Output:** After the reasoning, provide the final output strictly in the format below:

        Truth Score: [Provide a numerical score from 0 (Definitely False) to 100 (Definitely True). Use exactly 50 for Uncertain/Cannot Verify/Opinion. Base the score strictly on your reasoned analysis.]
        Sources: [List up to 2 sources directly supporting your conclusion. For EACH source, provide title and URL strictly as:
        Source Title: [Actual title of the page/article]
        Source URL: [Full, currently accessible URL]
        Ensure titles and URLs are accurate. If no reliable sources found or claim is opinion/unverifiable, write "None".]
    """
    # --- END UPDATED PROMPT ---

    try:
        generation_config_factcheck = genai.types.GenerationConfig(
            temperature=0.1
        )

        response = llm_model.generate_content(
            prompt,
            generation_config=generation_config_factcheck
        )

        if not response.candidates:
            try:
                block_reason = response.prompt_feedback.block_reason
                logging.warning(f"Fact-check prompt blocked. Reason: {block_reason}")
                return "Blocked", f"Content blocked by safety filter ({block_reason})", []
            except (AttributeError, ValueError, Exception):
                logging.error("Fact-check response empty or invalid, no candidates.", exc_info=True)
                return "N/A", "No valid response generated by the AI model.", []
        if not response.text:
                logging.warning("Fact-check response text is empty.")
                return "N/A", "Empty response text from the AI model.", []

        result_text: str = response.text.strip()

        # --- UPDATED PARSING ---
        truth_score: str = "50"
        explanation: str = "Could not parse reasoning from AI response." # Default if reasoning parse fails
        sources: List[SourceInfo] = []

        # 1. Extract Detailed Reasoning
        reasoning_match = re.search(r"1\.\s*\*\*Reasoning:\*\*(.*?)\n+--- END REASONING ---", result_text, re.IGNORECASE | re.DOTALL)
        if reasoning_match:
            explanation = reasoning_match.group(1).strip() # Use detailed reasoning as explanation
        else:
             # Fallback: try to grab explanation if reasoning marker is missing
             exp_fallback_match = re.search(r"^Explanation:\s*(.*?)(?=^\s*Sources:|$)", result_text, re.IGNORECASE | re.DOTALL | re.MULTILINE)
             if exp_fallback_match:
                 explanation = exp_fallback_match.group(1).strip() + " (Warning: Could not parse detailed reasoning section)"


        # 2. Extract Truth Score (from Formatted Output section)
        # Search after the reasoning marker if found, otherwise search whole text
        search_area = result_text.split("--- END REASONING ---", 1)[-1] if reasoning_match else result_text
        score_match = re.search(r"^Truth Score:\s*(\d+)", search_area, re.IGNORECASE | re.MULTILINE)
        if score_match:
            truth_score = score_match.group(1)

        # 3. Extract Sources (Title and URL pairs)
        sources_block_match = re.search(r"^Sources:\s*(.*)", search_area, re.IGNORECASE | re.DOTALL | re.MULTILINE)
        if sources_block_match:
            sources_text = sources_block_match.group(1).strip()
            if sources_text.lower() != 'none':
                # Find all Title/URL pairs
                found_sources = re.findall(
                    r"Source Title:(.*?)\n+Source URL:(.*?)(?:\n+Source Title:|\n*$)",
                    sources_text,
                    re.IGNORECASE | re.DOTALL
                )
                for title, url in found_sources:
                    cleaned_title = title.strip()
                    cleaned_url = url.strip()
                    if cleaned_title and cleaned_url:
                        sources.append({'title': cleaned_title, 'url': cleaned_url})
        # --- END UPDATED PARSING ---

        return truth_score, explanation, sources

    except Exception as e:
        logging.error(f"Error during Gemini API call in verify_fact", exc_info=True)
        error_details: str = str(e)
        return "Error", f"An API error occurred: {error_details}", []


# --- analyze_with_llm function (remains the same) ---
def analyze_with_llm(image_path: str) -> Dict[str, Any]:
    """ Analyze image authenticity using the configured LLM (Gemini). """
    # (Code is identical to previous version - no changes needed here)
    if not llm_model: logging.warning("analyze_with_llm called but LLM model is not available."); return {"type": "deepfake_detection", "error": "LLM model is not configured or failed to load."}
    if not os.path.exists(image_path): logging.error(f"Image file not found for LLM analysis: {image_path}"); return {"type": "deepfake_detection", "error": f"Image file not found for LLM: {os.path.basename(image_path)}"}
    image_file = None
    try:
        logging.info(f"Uploading image {os.path.basename(image_path)} to Gemini..."); image_file = genai.upload_file(path=image_path); logging.info(f"LLM Upload successful. File name: {image_file.name}")
        prompt = """Analyze this image for authenticity. Does it appear to be a real photograph, or show signs of being AI-generated, a deepfake, or otherwise manipulated?\n\nProvide your assessment strictly in the following format, clearly labeling each section:\nOverall Impression: [Choose one from: Real / Likely Real / Uncertain / Likely Fake / Fake]\nConfidence Score: [Provide a numerical score from 0 (Very Low Confidence) to 100 (Very High Confidence) for your Overall Impression]\nExplanation:\n[Provide a concise bulleted list with exactly 4 distinct points. Each point should specifically explain *why* the image appears fake (if your impression leans fake) or real (if your impression leans real), directly relating to the implied fake/real percentages derived from your Confidence Score and Impression. Ensure these points justify the scores you would assign.]\n- [Point 1: Reason justifying the score/impression]\n- [Point 2: Reason justifying the score/impression]\n- [Point 3: Reason justifying the score/impression]\n- [Point 4: Reason justifying the score/impression]\n\nExamples of cues to consider include unnatural textures, symmetries, lighting inconsistencies, strange details in eyes/hands/background, rendering artifacts typical of synthetic media, or conversely, signs of genuine photography like natural imperfections, film grain, expected distortions, etc."""
        logging.info("Sending image and prompt to Gemini LLM..."); response = llm_model.generate_content([prompt, image_file], generation_config=genai.types.GenerationConfig(temperature=0.1)); logging.info("Received response from Gemini LLM.")
    except Exception as e: logging.error(f"Error during LLM upload or API call: {e}", exc_info=True); return {"type": "deepfake_detection", "error": f"An API error occurred during LLM analysis: {str(e)}"}
    finally:
        if image_file:
            try: logging.info(f"Attempting to delete uploaded LLM file: {image_file.name}"); genai.delete_file(image_file.name); logging.info(f"Deleted uploaded LLM file: {image_file.name}")
            except Exception as del_e: logging.error(f"Error deleting LLM uploaded file {image_file.name}: {del_e}", exc_info=True)
    if not response.candidates:
        try: block_reason = response.prompt_feedback.block_reason; logging.warning(f"Deepfake LLM prompt blocked. Reason: {block_reason}"); return {"type": "deepfake_detection", "error": f"LLM analysis blocked by safety filter ({block_reason})"}
        except (AttributeError, ValueError, Exception): logging.error("Deepfake LLM response empty or invalid, no candidates.", exc_info=True); return {"type": "deepfake_detection", "error": "No valid LLM response generated."}
    if not response.text: logging.warning("Deepfake LLM response text is empty."); return {"type": "deepfake_detection", "error": "Empty response text from LLM."}
    result_text: str = response.text.strip(); impression: str = "N/A - Parse Error"; confidence_str: str = "N/A"; explanation: str = "Could not parse explanation from LLM response."
    impression_match = re.search(r"^Overall Impression:\s*(.*)", result_text, re.IGNORECASE | re.MULTILINE);
    if impression_match: impression = impression_match.group(1).strip()
    confidence_match = re.search(r"^Confidence Score:\s*(\d+)", result_text, re.IGNORECASE | re.MULTILINE);
    if confidence_match: confidence_str = confidence_match.group(1)
    exp_match = re.search(r"^Explanation:\s*(.*)", result_text, re.IGNORECASE | re.DOTALL | re.MULTILINE);
    if exp_match: explanation = exp_match.group(1).strip()
    llm_fake_score: int | str = "N/A"; llm_real_score: int | str = "N/A"
    try:
        conf_val: int = int(confidence_str); imp_lower = impression.lower()
        if "fake" in imp_lower: llm_fake_score = min(100, max(0, conf_val)); llm_real_score = 100 - llm_fake_score
        elif "real" in imp_lower: llm_real_score = min(100, max(0, conf_val)); llm_fake_score = 100 - llm_real_score
        else: llm_fake_score = 50; llm_real_score = 50
    except (ValueError, TypeError): llm_fake_score = 50; llm_real_score = 50; logging.warning(f"Could not parse LLM confidence '{confidence_str}'. Defaulting scores to 50/50.")
    return {"type": "deepfake_detection", "impression": impression, "llm_explanation": explanation, "llm_fake_score": llm_fake_score, "llm_real_score": llm_real_score}


# --- Main Image Detection Function ---
def detect_deepfake(image_path: str) -> Dict[str, Any]:
    """ Performs image authenticity analysis using ONLY the Google LLM. """
    # (Code is identical to previous version - no changes needed here)
    if not os.path.exists(image_path): logging.error(f"Image file not found on server before analysis: {image_path}"); return {"type": "deepfake_detection", "error": "Image file not found on server."}
    logging.info(f"Starting LLM analysis for image: {os.path.basename(image_path)}"); llm_result = analyze_with_llm(image_path); logging.info(f"LLM analysis complete for {os.path.basename(image_path)}. Returning result."); return llm_result


# --- evaluate_research_paper function (remains the same prompt-wise as last update) ---
def evaluate_research_paper(text: str) -> Tuple[int | str, str]:
    """ Evaluate a research paper's text using the configured LLM. """
    # (Code is identical to previous version - no changes needed here)
    if not llm_model: logging.warning("evaluate_research_paper called but LLM model is not available."); return "N/A", "LLM model is not configured or failed to load."
    MAX_INPUT_CHARS: int = 30000; truncated: bool = False; text_to_process: str
    if len(text) > MAX_INPUT_CHARS: logging.warning(f"Input text length ({len(text)}) > {MAX_INPUT_CHARS}. Truncating for evaluation."); text_to_process = text[:MAX_INPUT_CHARS]; truncated = True
    else: text_to_process = text
    prompt = f"""Act as an impartial academic reviewer. Evaluate the quality of the following research paper text based *only* on the provided text and these criteria:\n1. Clarity of Research Question/Purpose: Is the main goal clearly stated and understandable?\n2. Soundness of Methodology: Are the methods described adequately and appropriate for the question? (Assess based *only* on description provided).\n3. Significance & Validity of Findings/Conclusions: Are results clearly presented? Do they directly address the research question? Are conclusions strongly justified by the results/evidence presented? Is the significance discussed realistically?\n4. Overall Structure & Clarity of Writing: Is the paper well-organized, logical, and written clearly and concisely?\n\nProvide your response STRICTLY in the following format:\n\nScore Percent: [Assign an overall quality score percentage from 0% to 100% based ONLY on the provided text and the 4 criteria above. Be critical and consistent.]\nJustification: [First, list specific 'Strengths:' observed based *only* on the text and the criteria (e.g., "Clear statement of purpose"). Then, list specific 'Weaknesses:' observed based *only* on the text and the criteria (e.g., "Methodology lacks detail", "Conclusion overstates findings"). Finally, provide a concluding sentence explaining how these specific strengths and weaknesses justify the exact 'Score Percent' assigned above.]{' Note: Evaluation based on truncated text.' if truncated else ''}\n\n--- START RESEARCH PAPER TEXT ---\n{text_to_process}\n--- END RESEARCH PAPER TEXT ---"""
    try:
        generation_config = genai.types.GenerationConfig(temperature=0.1)
        response = llm_model.generate_content(prompt, generation_config=generation_config)
        if not response.candidates:
            try: block_reason = response.prompt_feedback.block_reason; logging.warning(f"Evaluation prompt blocked. Reason: {block_reason}"); return "Blocked", f"Content blocked by safety filter ({block_reason})"
            except (AttributeError, ValueError, Exception): logging.error("Evaluation response empty or invalid, no candidates.", exc_info=True); return "N/A", "No valid response generated by the AI model for evaluation."
        if not response.text: logging.warning("Evaluation response text is empty."); return "N/A", "Empty response text from the AI model for evaluation."
        result_text: str = response.text.strip(); score_percent: int | str = "N/A"; justification: str = "Could not parse justification from AI response."
        score_match = re.search(r"^Score Percent:\s*(\d{1,3})\s*%?", result_text, re.IGNORECASE | re.MULTILINE)
        if score_match: score_value: int = int(score_match.group(1)); score_percent = max(0, min(100, score_value))
        just_match = re.search(r"^Justification:\s*(.*)", result_text, re.IGNORECASE | re.DOTALL | re.MULTILINE)
        if just_match:
            justification = just_match.group(1).strip()
            if truncated and "[Text Truncated]" not in justification and "truncated text" not in justification.lower(): justification += "\n\n(Note: This evaluation was based on truncated text due to length limitations.)"
        return score_percent, justification
    except Exception as e:
        logging.error(f"Error during Gemini API call in evaluate_research_paper", exc_info=True)
        return "Error", f"An API error occurred during paper evaluation: {str(e)}"
